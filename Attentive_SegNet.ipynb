{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soulsharp/Attentive-Segnet/blob/main/Attentive_SegNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrIycxsQ1hT6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "import os\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset for segmentation tasks.\n",
        "\n",
        "        Args:\n",
        "            image_dir (str): Path to the directory containing RGB images.\n",
        "            mask_dir (str): Path to the directory containing mask images.\n",
        "            transform (callable, optional): A function/transform to apply to both images and masks.\n",
        "        \"\"\"\n",
        "        self.image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "        self.mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir) if fname.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "        if len(self.image_paths) != len(self.mask_paths):\n",
        "            raise ValueError(\"Number of images and masks must be the same!\")\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Loads image and mask\n",
        "        image_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        image = cv.imread(image_path)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "        if mask is None:\n",
        "            raise ValueError(f\"Failed to load mask: {mask_path}\")\n",
        "\n",
        "        # Applies transformations\n",
        "        if self.transform:\n",
        "            image, mask = self.transform(image, mask)\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = torch.as_tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
        "        mask = torch.as_tensor(mask, dtype=torch.long)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Simple preprocessing function\n",
        "def transform_image_and_mask(image, mask, size=512):\n",
        "    # Resize image and mask\n",
        "    image = cv.resize(image, (size, size), interpolation=cv.INTER_NEAREST)\n",
        "    mask = cv.resize(mask, (size, size), interpolation=cv.INTER_NEAREST)\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "_ti1LYG7irdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def make_train_val_loaders(image_directory, mask_directory):\n",
        "  # Instantiates dataset\n",
        "  dataset = CrackDataset(image_dir=image_directory, mask_dir=mask_directory, transform=transform_image_and_mask)\n",
        "\n",
        "  # Train-validation split\n",
        "  train_ratio = 0.8\n",
        "  train_size = int(len(dataset) * train_ratio)\n",
        "  val_size = len(dataset) - train_size\n",
        "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  # Creates DataLoaders\n",
        "  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "  return train_loader, val_loader"
      ],
      "metadata": {
        "id": "gZ3cdLrOCtua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K-5mDs8sat9",
        "outputId": "7db397c5-f81d-4134-8a7a-2a04462b607d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 256, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Encodes patch representation of an image batch\n",
        "class Patch_Embedding(nn.Module):\n",
        "    def __init__(self, embed_dim=64, patch_size=32, in_channels=3):\n",
        "        super(Patch_Embedding, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.convolution = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Expects x to be of shape (batch_size, channels, height, width)\n",
        "        h, w = x.shape[2], x.shape[3]\n",
        "        if h % self.patch_size != 0 or w % self.patch_size != 0:\n",
        "          raise ValueError(\"Input dimensions must be divisible by the patch size.\")\n",
        "        # Applies convolution to create patches\n",
        "        patches = self.convolution(x)\n",
        "\n",
        "        return patches\n",
        "\n",
        "embedding_dim = 64\n",
        "# patch information contains the patch embeddings of all training samples\n",
        "patch_information = []\n",
        "patch_embedding = Patch_Embedding()\n",
        "\n",
        "# Tensor for validating the entire process\n",
        "test_tensor = torch.randn((2, 3, 512, 512))\n",
        "output_patches = patch_embedding(test_tensor)\n",
        "output_patch_encoding = output_patches.view(output_patches.size(0), -1, embedding_dim)\n",
        "\n",
        "output_patch_encoding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tro-lDSES7x"
      },
      "outputs": [],
      "source": [
        "# returns a tensor after applying maxpool2d\n",
        "# also returns the indices used up in the max pool\n",
        "class MaxPoolWithIndices(nn.Module):\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(MaxPoolWithIndices, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=self.kernel_size, stride=self.stride, return_indices=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, indices = self.maxpool(x)\n",
        "        return output, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OodSqmZql7Ck"
      },
      "outputs": [],
      "source": [
        "# basic convblock with conv2d-->batchnorm-->relu-->maxpool(optional)\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, maxpool_flag=True):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.maxpool_flag = maxpool_flag\n",
        "        self.operation_seq = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        if self.maxpool_flag:\n",
        "            self.maxpool = MaxPoolWithIndices(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.operation_seq(x)\n",
        "\n",
        "        if self.maxpool_flag:\n",
        "            pooled, indices = self.maxpool(x)\n",
        "            return pooled, indices\n",
        "        else:\n",
        "            return x, None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Downsample(input_tensor):\n",
        "    max_indices = {}\n",
        "    bottleneck_input = None\n",
        "\n",
        "    assert input_tensor.shape[-2] % 4 == 0 and input_tensor.shape[-1] % 4 == 0, \\\n",
        "        \"Input dimensions must be divisible by 4 for this downsampling pipeline.\"\n",
        "\n",
        "    conv_blocks = [\n",
        "        (\"block1\", ConvBlock(3, 8, 3, 1, 1)),\n",
        "        (\"block2\", ConvBlock(8, 16, 3, 1, 1)),\n",
        "        (\"block3\", ConvBlock(16, 32, 3, 1, 1)),\n",
        "        (\"block4\", ConvBlock(32, 64, 3, 4, 1, False)),\n",
        "    ]\n",
        "\n",
        "    for name, conv_block in conv_blocks:\n",
        "        input_tensor, indices = conv_block(input_tensor)\n",
        "\n",
        "        if name == \"block4\":\n",
        "            bottleneck_input = input_tensor\n",
        "\n",
        "        else:\n",
        "          max_indices[name] = indices\n",
        "\n",
        "    return max_indices, bottleneck_input"
      ],
      "metadata": {
        "id": "ntOs6m_PEDLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzwfjl1MWO4o"
      },
      "outputs": [],
      "source": [
        "# Attention weights to scale the patch embed representation\n",
        "class AttentionWeights(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(AttentionWeights, self).__init__()\n",
        "        self.alphas = nn.Parameter(torch.randn(output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # adding a dimension ie making alphas (256, 1) for proper broadcasting\n",
        "        scaled_input = self.alphas.unsqueeze(1) * x\n",
        "        return scaled_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGM_8n48B9Xm"
      },
      "outputs": [],
      "source": [
        "# Takes in the tensor containing patch information of all images in the batch\n",
        "# Applies attention scaling on each image and returns BATCH_SZ X 256 X 64 tensor\n",
        "def attention_to_all_images(patch_emb_tensor, attention_dim):\n",
        "    att_weights = AttentionWeights(attention_dim)\n",
        "    attended_inputs = att_weights(patch_emb_tensor)\n",
        "\n",
        "    return attended_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xPWxsxOGx-H"
      },
      "outputs": [],
      "source": [
        "# Takes in the bottleneck tensor and adds patch embeddings to it\n",
        "def add_bottleneck_and_patch_embeddings(bottleneck, patch_embedding):\n",
        "\n",
        "  num_batches, num_channels, h, w = bottleneck.shape\n",
        "  patch_embedding = patch_embedding.permute(0, 2, 1)\n",
        "  patch_embedding = patch_embedding.view(num_batches, num_channels, h, w)\n",
        "\n",
        "  return bottleneck + patch_embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(UpBlock, self).__init__()\n",
        "        self.operation_sequence = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.operation_sequence(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "C_93qBvHHGAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unpool2d(nn.Module):\n",
        "  def __init__(self, k_sz, stride):\n",
        "    super(Unpool2d, self).__init__()\n",
        "    self.unpool2d = nn.MaxUnpool2d(k_sz, stride)\n",
        "\n",
        "  def forward(self, pooled, indices):\n",
        "    return self.unpool2d(pooled, indices)"
      ],
      "metadata": {
        "id": "zWAizrIRfQdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5MZO-vwXYJf"
      },
      "outputs": [],
      "source": [
        "def Upsample(input_tensor, max_indices):\n",
        "    up_blocks = [\n",
        "        UpBlock(64, 32, kernel_size=4, stride=4, padding=-0),\n",
        "        Unpool2d(2, 2),\n",
        "        UpBlock(32, 16, kernel_size=3, stride=1, padding=1),\n",
        "        Unpool2d(2, 2),\n",
        "        UpBlock(16, 8, kernel_size=3, stride=1, padding=1),\n",
        "        Unpool2d(2, 2),\n",
        "        UpBlock(8, 1, kernel_size=3, stride=1, padding=1),\n",
        "    ]\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for idx, up_block in enumerate(up_blocks):\n",
        "\n",
        "      if idx % 2 == 0:\n",
        "        input_tensor = up_block(input_tensor)\n",
        "      else:\n",
        "        max_index_key = \"block\" + str(3-count)\n",
        "        input_tensor = up_block(input_tensor, max_indices[max_index_key])\n",
        "        count += 1\n",
        "\n",
        "    return input_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class weightedBCEDiceLoss(nn.Module):\n",
        "  def __init__(self, weight_BCE, weight_Dice, epsilon):\n",
        "    super(weightedBCEDiceLoss, self).__init__()\n",
        "    self.alpha = weight_BCE\n",
        "    self.beta = weight_Dice\n",
        "    self.eps = epsilon\n",
        "    assert self.alpha + self.beta == 1.0 , \"weights of BCE loss and Dice Loss must sum to 1\"\n",
        "\n",
        "  def forward(self, preds, targets):\n",
        "    preds = torch.sigmoid(preds).view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    bce_criterion = nn.BCELoss()\n",
        "    bce_loss = bce_criterion(preds, targets)\n",
        "\n",
        "    intersection = torch.sum(preds * targets)\n",
        "    union = torch.sum(preds) + torch.sum(targets)\n",
        "    dice_coeff = (2 * intersection + self.eps) / (union + self.eps)\n",
        "    dice_loss = 1 - dice_coeff\n",
        "\n",
        "    total_loss = self.alpha * bce_loss + self.beta * dice_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "rTXxl7Noivj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = torch.randn(2, 1, 256, 256)\n",
        "target = torch.randint(0, 2, (2, 1, 256, 256)).float()\n",
        "\n",
        "loss_fn  = weightedBCEDiceLoss(0.6, 0.4, 0.001)\n",
        "loss_fn(pred, target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RN5jO1YwFo_",
        "outputId": "ec6ee7f8-7e47-4e14-e0fb-ce409bcabc52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6630)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "\n",
        "    \"embed_dim\":64,\n",
        "    \"patch_size\":32,\n",
        "    \"in_channels\":3,\n",
        "    \"weight_bce\": 0.5,\n",
        "    \"weight_dice\": 0.5,\n",
        "    \"epsilon_loss_fn\": 1e6,\n",
        "    \"train_batch_size\": 64,\n",
        "    \"train_epochs\": 100,\n",
        "    \"lr\": 0.001,\n",
        "    \"ckpt_name\": 'att_segnet.pth',\n",
        "    \"model_save_path\":\"/content/saved_models\"\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "OqruSU3S6fT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentiveSegNet(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(AttentiveSegNet, self).__init__()\n",
        "    self.patch_generator = Patch_Embedding(config[\"embed_dim\"], config[\"patch_size\"], config[\"in_channels\"])\n",
        "    self.embed_dim = config['embed_dim']\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Computes patch encodings for the input tensor x and scales the representations\n",
        "    patched_encodings = self.patch_generator(x)\n",
        "    patched_encodings = patched_encodings.view(patched_encodings.size(0), -1, self.embed_dim)\n",
        "    _, attention_dimension, _ = patched_encodings.shape\n",
        "    scaled_embeddings = attention_to_all_images(patched_encodings, attention_dimension)\n",
        "\n",
        "    # Downsamples the input_tensor x\n",
        "    max_indices, bottleneck_input = Downsample(x)\n",
        "\n",
        "    # Fuses patch information with downsampled information\n",
        "    bottleneck_plus_emb = add_bottleneck_and_patch_embeddings(bottleneck_input, scaled_embeddings)\n",
        "\n",
        "    # Upscales the bottleneck input and gets the segmentation map\n",
        "    output_mask = Upsample(bottleneck_plus_emb, max_indices)\n",
        "\n",
        "    return output_mask\n"
      ],
      "metadata": {
        "id": "XLxaHDIT6F5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_for_one_epoch(epoch_idx, model, train_loader, optimizer, config):\n",
        "\n",
        "    r\"\"\"\n",
        "    Method to run the training for one epoch.\n",
        "    :param epoch_idx: iteration number of current epoch\n",
        "    :param model: Attentive SegNet model\n",
        "    :param train_loader: Dataloader for the training set\n",
        "    :param optimizer: optimizer to be used\n",
        "    :param config: config dictionary that contains model information\n",
        "    :return: loss value for the epoch\n",
        "    \"\"\"\n",
        "\n",
        "    losses = []\n",
        "    criterion = weightedBCEDiceLoss(config[\"weight_bce\"], config[\"weight_dice\"], config[\"epsilon_loss_fn\"])\n",
        "\n",
        "    # Iterates through the dataloader in form of batches\n",
        "    for batch in tqdm(train_loader):\n",
        "        im, mask = batch\n",
        "        im = im.to(device)\n",
        "        mask = mask.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        model_output = model(im)\n",
        "        loss = criterion(model_output, mask)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Prints epoch and loss info\n",
        "    print('Finished epoch: {} | Number Loss : {:.4f}'.\n",
        "          format(epoch_idx + 1,\n",
        "                 np.mean(losses)))\n",
        "    return np.mean(losses)\n",
        "\n",
        "def train(train_loader, config):\n",
        "    model = AttentiveSegNet(config).to(device)\n",
        "    num_epochs = config['train_epochs']\n",
        "    optimizer = Adam(model.parameters(), lr=config['lr'])\n",
        "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "    # Creates output directories\n",
        "    if not os.path.exists(config['model_save_path']):\n",
        "        os.mkdir(config['model_save_path'])\n",
        "\n",
        "    # Loads checkpoint if found\n",
        "    current_dir = os.get_cwd()\n",
        "    if os.path.exists(os.path.join(config['model_save_path'], config['ckpt_name'])):\n",
        "        print('Loading checkpoint')\n",
        "        model.load_state_dict(torch.load(os.path.join(config['model_save_path'],\n",
        "                                    config['ckpt_name']), map_location=device))\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        mean_loss = train_for_one_epoch(epoch_idx, model, train_loader, optimizer, config)\n",
        "        scheduler.step(mean_loss)\n",
        "\n",
        "        # Updates checkpoint if better model params found\n",
        "        if mean_loss < best_loss:\n",
        "            print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n",
        "            torch.save(model.state_dict(), os.path.join(config['model_save_path'],\n",
        "                                            config['ckpt_name']))\n",
        "            best_loss = mean_loss\n",
        "        else:\n",
        "            print('No Loss Improvement')"
      ],
      "metadata": {
        "id": "Esj0UYB0GIEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_segmentation_accuracy(preds, targets):\n",
        "  preds = preds.view(-1)\n",
        "  targets = targets.view(-1)\n",
        "\n",
        "  matches = (preds == targets).sum().item()\n",
        "  accuracy = matches / preds.shape(0)\n",
        "  return matches\n",
        "\n",
        "def inference(val_loader, config):\n",
        "   model = AttentiveSegNet(config).to(device)\n",
        "   model.eval()\n",
        "\n",
        "   # Loads checkpoint if found\n",
        "   if os.path.exists(os.path.join(config['model_save_path'], config['ckpt_name'])):\n",
        "        print('Loading checkpoint')\n",
        "        model.load_state_dict(torch.load(os.path.join(config['model_save_path'],\n",
        "                                     config['ckpt_name']), map_location=device))\n",
        "   else:\n",
        "        print('No checkpoint found at {}'.format(os.path.join(config['model_save_path'],\n",
        "                                        config['ckpt_name'])))\n",
        "\n",
        "   for idx, batch in enumerate(tqdm(val_loader)):\n",
        "        im, mask = batch\n",
        "        im = im.to(device)\n",
        "        mask = mask.to(device)\n",
        "        preds = model(im)\n",
        "        preds = preds.to(device)\n",
        "\n",
        "        accuracy = get_segmentation_accuracy(preds, mask)\n",
        "        print(f\"Accuracy in batch {idx} during validation is: {accuracy}\")"
      ],
      "metadata": {
        "id": "zo5uJJi5KFW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_directory = \"\"\n",
        "mask_directory = \"\"\n",
        "train_loader, val_loader = make_train_val_loaders(image_directory, mask_directory)"
      ],
      "metadata": {
        "id": "pgUivfFuQnx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader, config)\n",
        "inference(val_loader, config)"
      ],
      "metadata": {
        "id": "OyRnnH6NRSZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgJWKt/jx8iJB/rMJChEbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}